{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proje.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "t7n30sUaisVN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**Reading the Data**\n",
        "\n",
        "The data is read  from a file inside the drive into a Pandas DataFrame. Each row contains a sentence and the label of the sentence, whether it is positive or negative."
      ]
    },
    {
      "metadata": {
        "id": "4KcBODWOimLX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get the lines from the file\n",
        "yelp_file = open('yelp_labelled.txt', 'r')\n",
        "yelp_lines = yelp_file.readlines()\n",
        "yelp_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rA25EOK_l6WF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Construct the DataFrame\n",
        "import pandas as pd\n",
        "yelp = pd.DataFrame(data=None, index=None, columns=['Sentence','Value'], dtype=None, copy=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIB73svRlEAm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Convert each line to an appropriate row\n",
        "for line in yelp_lines:\n",
        "  values = line.split(\"\\t\")\n",
        "  row = pd.Series([values[0],values[1][:1]],index=['Sentence', 'Value'])\n",
        "  yelp = yelp.append(row, ignore_index = True)\n",
        "  \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qg2vnN_Pmz2v",
        "colab_type": "code",
        "outputId": "cb8e6124-1838-4c3a-fd1c-b6194cc4d309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "#Head of the DF\n",
        "yelp.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence Value\n",
              "0                           Wow... Loved this place.     1\n",
              "1                                 Crust is not good.     0\n",
              "2          Not tasty and the texture was just nasty.     0\n",
              "3  Stopped by during the late May bank holiday of...     1\n",
              "4  The selection on the menu was great and so wer...     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "IzsSy6XZP0ce",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Divide train and validation data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = yelp.drop(['Value'],axis=1)\n",
        "Y = yelp['Value']\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H6m0qxqLjpeq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**\n",
        "\n",
        "We have to transform our data to a format eligible for analysing. \n",
        "\n",
        "\n",
        "1.   Case Conversion: Convert every letter to lowercase.\n",
        "2.   Remove Punctuation\n",
        "3.   Remove Stop Words: Removes the commonly used words which has no sentimental meaning like a, an, the.\n",
        "4.   Lemmatization: Deals with removal of inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
        "5.   Spelling Correction\n",
        "6.   Part of Speech Tagging: Classify and tag each word as verb, noun, etc.\n",
        "7.   Remove Numbers\n",
        "\n",
        "Below, these are implemented as functions, and there is the myDataProcessing() function which performs all operations mentioned above.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7xSV7ygGk-yU",
        "colab_type": "code",
        "outputId": "0b0dc69b-f307-48b8-be86-8d93bcb542a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "cell_type": "code",
      "source": [
        "#Import, install and download the necessary libraries.\n",
        "!pip install pyspellchecker \n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from spellchecker import SpellChecker\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "EiYPR4aFvPww",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get the list of stop words from NLTK\n",
        "stopWords = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CsrSiDYKEc96",
        "colab_type": "code",
        "outputId": "962c78bc-4c84-42ec-bb1d-99ccf81439be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Let us see if the word 'also' is a member of stop words\n",
        "\"also\" in stopWords "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "GZ4OebOtnPOX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Input: Lowercase string without punctuation\n",
        "#Output: Array of words\n",
        "#Deletes the stopwords except \"not\". 'Not' is essentially one of the stop words in our corpus. However, we believe that it has an importance on deciding the content of a comment.\n",
        "def RSW(myString):\n",
        "  stringArray = myString.split()\n",
        "  newArray = []\n",
        "  for word in stringArray:\n",
        "    if word not in stopWords or word == \"not\":\n",
        "      newArray.append(word)\n",
        "  return newArray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OKjPPI1Bve1G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Input: Array of words\n",
        "#Output: Array of words\n",
        "#Corrects the misspelled words\n",
        "def mySpellChecker(stringArray):\n",
        "  spell = SpellChecker()\n",
        "  newArray = []\n",
        "  for word in stringArray:\n",
        "    misspelled = spell.unknown(word)\n",
        "    if not misspelled:\n",
        "      newArray.append(word)\n",
        "    else:\n",
        "      newArray.append(spell.correction(word))\n",
        "      \n",
        "  return newArray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ErZ9sT7Rx-eY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Input: Lowercase string without punctuation\n",
        "#Output: Array of words\n",
        "#Removes the stop words, checks the spelling, corrects if necessary, lemmatizes.\n",
        "def RSWandSCandLM(myString):\n",
        "  spell = SpellChecker()\n",
        "  wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
        "  stringArray = myString.split()\n",
        "  newArray = []\n",
        "  for word in stringArray:\n",
        "    if word not in stopWords or word == \"not\":\n",
        "      misspelled = spell.unknown(word)\n",
        "      if not misspelled:\n",
        "        newArray.append(wordnet_lemmatizer.lemmatize(word))\n",
        "      else:\n",
        "        wordCorrected = spell.correction(word)\n",
        "        newArray.append(wordnet_lemmatizer.lemmatize(wordCorrected))\n",
        "  return newArray\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n1Ye28Rs0oiF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Prequisite: Function RSWandSCandLM()\n",
        "#Input: DataFrame storing the sentence in column \"Sentence\"\n",
        "#Output: DataFrame\n",
        "#This is the final preprocession function: lowercase+ strip punctuation+spellcheck+remove stop words+lemmatize+add part of speech tags+remove numbers\n",
        "#tags: https://www.nltk.org/book/ch05.html\n",
        "def myDataPreprocessing(myDataFrame):\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  translator2 = str.maketrans(string.ascii_letters, string.ascii_letters, string.digits)\n",
        "  wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
        "  \n",
        "  myDataFrame['POStags'] = [None] * myDataFrame.shape[0]\n",
        "\n",
        "  for index, row in myDataFrame.iterrows():\n",
        "    row.Sentence = row.Sentence.lower()\n",
        "    row.Sentence = row.Sentence.translate(translator)\n",
        "    row.Sentence = row.Sentence.translate(translator2)\n",
        "    row.Sentence = RSWandSCandLM(row.Sentence)\n",
        "    row.POStags = nltk.pos_tag(row.Sentence)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLfSqL-828T4",
        "colab_type": "code",
        "outputId": "0b514057-f55c-4c81-f221-19d29215b36d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        }
      },
      "cell_type": "code",
      "source": [
        "#Apply preprocessing\n",
        "myDataPreprocessing(x_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-74ea1fd2760b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDataPreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-f691987abfd0>\u001b[0m in \u001b[0;36mmyDataPreprocessing\u001b[0;34m(myDataFrame)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRSWandSCandLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOStags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-578e66ff8722>\u001b[0m in \u001b[0;36mRSWandSCandLM\u001b[0;34m(myString)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRSWandSCandLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mspell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mwordnet_lemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mstringArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyString\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnewArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, local_dictionary, distance)\u001b[0m\n\u001b[1;32m     40\u001b[0m                        'exist!').format(language.lower())\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_frequency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36mload_dictionary\u001b[0;34m(self, filename, encoding)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36m_update_dictionary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_letters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dictionary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_letters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "NAcYU6sS5n6w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Show head of data\n",
        "x_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vP96QwQlJKXK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Sentiment techniques can be classified into 2:**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*a)Lexicon Based Approach*\n",
        "\n",
        "It is based on finding the opinion lexicon for calculating the sentiment for a given text.  It deals with counting the number of positive and negative words in the text. If the text consists of more positive words, the text is assigned a positive score. If there are more number of negative words the text is assigned a negative score. If the text contains equal number of positive and negative words then it is assigned a neutral score. To determine whether a word is positive or negative an opinion lexicon (positive and negative opinion words) is built. We used *'sentiword'* as our corpus. We developed some rule-based approaches particular for this dataset. \n",
        "\n",
        "b) *Machine Learning Approach *\n",
        "\n",
        "Here, two sets of documents are needed: training and a test set. A supervised learning classifier uses the training set to learn and train itself with respect to the differentiating attributes of text, and the performance of the classifier is tested using test dataset. Several machine learning algorithms are used for classification of text.\n",
        "\n",
        "\n",
        "For Reference: (Anuja P Jain ; Padma Dandannavar, Application of Machine Learning Techniques to Sentiment Analysis  ) https://ieeexplore.ieee.org/document/7912076\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HImDV51qeT-C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Lexicon Based Approach**"
      ]
    },
    {
      "metadata": {
        "id": "8NX3FVmhdEtL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import sklearn.metrics as ms\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from spellchecker import SpellChecker\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "82aZCBb9dEtG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def penn_to_wn(tag):\n",
        "\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif tag.startswith('IN'):\n",
        "        return \"i\"\n",
        "    return None\n",
        "\n",
        "def swn_polarity(text): #a rule based function for extracting the polarity of the words\n",
        "\n",
        "    sentiment = 0.0\n",
        "    tokens_count = 0\n",
        "\n",
        "    text = text.replace(\"n't\", \" not\") #converts acronyms to 'not'\n",
        "    text = text.replace(\"/\", \" \") \n",
        "    text = text.replace(\"as well\", \"\")\n",
        "    text = text.replace(\"at best\", \"atbest\")\n",
        "    raw_sentences = sent_tokenize(text)\n",
        "    for raw_sentence in raw_sentences:\n",
        "        tagged_sentence = pos_tag(word_tokenize(raw_sentence.lower()))\n",
        "        arr = []\n",
        "        for i in tagged_sentence:\n",
        "            if len(RSW(i[0])) != 0:\n",
        "                arr.append(i)\n",
        "        tagged_sentence = arr\n",
        " \n",
        "        for word, tag in tagged_sentence:\n",
        "            if word.isdigit(): continue\n",
        "            \n",
        "            wn_tag = penn_to_wn(tag)\n",
        "            if word in dictionary:\n",
        "#                 print(word,sentiment)\n",
        "                sentiment += dictionary[word]\n",
        "             \n",
        "            elif (wn_tag == 'v' and word[-3:] == \"ing\" and len(wn.synsets(word)) != 0 ): \n",
        "                i = 0\n",
        "                found = 0\n",
        "                lst = wn.synsets(word)\n",
        "                while i < len(lst) and found == 0:\n",
        "                    if word in lst[i].name() and \".s.\" in lst[i].name():\n",
        "                        swn_synset = swn.senti_synset(lst[i].name())\n",
        "                        found = 1\n",
        "                    else:\n",
        "                        i += 1\n",
        "                if found == 0 and lst != []: #if the equivalent of the word is not found in sentiword\n",
        "                    synset = lst[0]\n",
        "                    swn_synset = swn.senti_synset(synset.name())\n",
        "                    arr = []\n",
        "                    tot = 0\n",
        "                    for i in lst:\n",
        "                        synset = swn.senti_synset(i.name())\n",
        "                        tot += synset.pos_score() - synset.neg_score()\n",
        "                    tot = tot / len(wn.synsets(word))\n",
        "#                     print(word,sentiment)  \n",
        "                    sentiment += tot\n",
        "                    continue\n",
        "#                 print(word,sentiment)   \n",
        "                sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
        "                \n",
        "                tokens_count += 1\n",
        "            \n",
        "            elif wn_tag == 'i' : continue\n",
        "            \n",
        "            else: \n",
        "                # if the word is not a verb\n",
        "                if len(wn.synsets(word)) == 0: \n",
        "                    continue   \n",
        "                    \n",
        "                #Here, we find the averages of each word's positive and negative scores in order to get a better result \n",
        "                arr = []\n",
        "                tot = 0\n",
        "                for i in wn.synsets(word):\n",
        "                    synset = swn.senti_synset(i.name())\n",
        "                    tot += synset.pos_score() - synset.neg_score()\n",
        "                tot = tot / len(wn.synsets(word))\n",
        "#                 print(word,tot)\n",
        "#                 print(word,sentiment)\n",
        "                sentiment += tot\n",
        "                \n",
        "                tokens_count += 1\n",
        " \n",
        "    # judgment call ? Default to positive or negative\n",
        "    if not tokens_count:\n",
        "        return 0\n",
        " \n",
        "    # sum greater than 0 => positive sentiment\n",
        "    if sentiment >= 0:\n",
        "        return 1\n",
        " \n",
        "    # negative sentiment\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rUZn1iN3eeAH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dictionary = {\n",
        "        \"else\" : -0.75,\n",
        "        \"better\" : -0.5,\n",
        "        \"generous\": 0.4,\n",
        "        \"other\": -0.5,\n",
        "        \"others\": -0.5,\n",
        "        \"another\": -0.4,\n",
        "        \"blah\": -0.5,\n",
        "        \"vegetarian\": 0,\n",
        "        \"atbest\": -0.5,\n",
        "        \"time\": 0,\n",
        "        \"rubber\": -0.2,\n",
        "        \"tasteless\": -0.3\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WdevbFm0gB6-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Predict on validation set\n",
        "preds_lex = []\n",
        "for row in x_valid['Sentence']:\n",
        "    preds = swn_polarity(row)\n",
        "    preds_lex.append(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lRY_NzwHRhwW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**A Small Visualization Using Wordcloud**\n",
        "\n",
        "A Wordcloud (or Tag cloud) is a visual representation of text data. It displays a list of words, the importance of each beeing shown with font size or color. This format is useful for quickly perceiving the most prominent terms.\n",
        "\n",
        "We wanted to add this figure to make our project more lively. We recognized some words like 'amazing, good, nice, wonderful' in positive labeled sentences and 'disappointed, terrible, won't' for negative labelled sentences. Thus, it gave us an initial understanding before going on with training and testing our data."
      ]
    },
    {
      "metadata": {
        "id": "IgmHNJnMBPiX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Feature Extraction**"
      ]
    },
    {
      "metadata": {
        "id": "mnd14uDlw1Gc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.classify import SklearnClassifier\n",
        "\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "train_pos = yelp[ yelp['Value'] == 1]\n",
        "train_neg = yelp[ yelp['Value'] == 0]\n",
        "\n",
        "\n",
        "def wordcloud_draw(data, color = 'black'):\n",
        "    words = ' '.join(data)\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                      background_color=color,\n",
        "                      width=2500,\n",
        "                      height=2000\n",
        "                     ).generate(str(x_train['Sentence']))\n",
        "    plt.figure(1,figsize=(13, 13))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "print(\"Positive words\")\n",
        "wordcloud_draw(train_pos,'white')\n",
        "print(\"Negative words\")\n",
        "wordcloud_draw(train_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F6Ztj9Q-8E5F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we skip to the ML Approach. We have to represent the document in a way that we can train models and obtain predictions."
      ]
    },
    {
      "metadata": {
        "id": "SE57OlGz0EIY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **1. Bag-Of-Words Representation**\n",
        "\n",
        "We will consider each sentence as a set of independent words. \n",
        "\n",
        "1. We will form a vocabulary by extracting unique words used. These words will be the columns of our training data.\n",
        "2. Every training sample will be represented by a row. For every sample, we will iterate through our vocabulary and count the occurences of each word.\n",
        "\n",
        "So (i,j)^th entry in our data will correspond to the count of word j in i^th sample.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GA_VUMA1Bj0Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get the distinct words from the training data.\n",
        "vocab = set()\n",
        "\n",
        "for index, row in x_train.iterrows():\n",
        "    vocab |= set(row.Sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSA767AtGg0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#How many words are there in the vocabulary\n",
        "len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Re7S5QR4Fj3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Input: A DataFrame storing the sentences in its 'Sentence' column.\n",
        "#Output: Bag of Words Representation of the data\n",
        "#Compute the Bag of Words Representation\n",
        "def ComputeBOW(data):\n",
        "  bow = pd.DataFrame(data=None, index=None, columns=vocab, dtype=None, copy=False)\n",
        "  \n",
        "  for index, row in data.iterrows():\n",
        "    counter = []\n",
        "  \n",
        "    for word in vocab:\n",
        "      counter.append(row.Sentence.count(word))\n",
        "    \n",
        "    newrow = pd.Series(counter, index = list(vocab))\n",
        "\n",
        "    bow = bow.append(newrow, ignore_index = True)\n",
        "    \n",
        "  return bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pm1H_wK34cWr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compute the Bag of Words representation of the train data.\n",
        "bow = ComputeBOW(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YUXt3BJE92bT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Look at our data\n",
        "bow.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4eFY00O_vaj3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as py\n",
        "print(\"Unique values in our data: \", py.unique(bow))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kaw67Kl1-c9F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This means, the maximum number of words repeated in a single sample is 4.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "S980s0ke0bGA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **2. TF-IDF Features**"
      ]
    },
    {
      "metadata": {
        "id": "VWbUEYuW-t6i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Term Frequency:** Number of occurences of term t in document d is defined as the number of times that t occurs in d. \n",
        "\n",
        "In the previous example, we saw that some terms occur once and some might occur 2, 3 or 4 times. If we use this representation, we would be treating like having more words and relevance of the result is direcly proportional. As if, having a word 2 times is 2 times relevant. To avoid this, we will check the Log Frequency of weight of the term. \n",
        "\n",
        "**Log Frequency Weight of a Term=** \n",
        "$1 + \\log_{10}TermFrequency$ \\\\\n",
        "If the TermFrequency is not 0, and 0 otherwise.\n",
        "\n",
        "Moreover, rare words are more important. For example, we remove the stop words as they are extremely common. Inverse Document Frequency can capture this. IDF is the frequency of the number of documents a word occurs in. We will again take the logarithm.\n",
        "\n",
        "**IDF= ** $\\log_{10} (N \\div TermFrequency)$ \\\\\n",
        "\n",
        "To benefit from both approached, we use TF-IDF, which is the product of log frequency weight of a word and IDF.\n",
        "\n",
        "**TF-IDF= ** $(1 + \\log_{10}TermFrequency) \\cdot  (\\log_{10} (N \\div TermFrequency))$ \\\\\n",
        "\n",
        "We have the term\\word frequencies computed in the Bag of Words approach, we can use it to calculate the TF-IDF feature."
      ]
    },
    {
      "metadata": {
        "id": "xA896_NB2cYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "#Input: The Bag of Words Model\n",
        "#Output: Log Frequency Weights of the BOW Model\n",
        "#Takes the log of each row if they are >0\n",
        "def LogFrequencies(DataFrame):\n",
        "  new = pd.DataFrame(data=None, index=None, columns=vocab, dtype=float, copy=False)\n",
        "  for index, row in DataFrame.iterrows():\n",
        "    newrow = []\n",
        "    for value in row:\n",
        "      if int(value) != 0:\n",
        "        newrow.append(float(1 + math.log10(float(value))))\n",
        "      else: \n",
        "        newrow.append(0)\n",
        "    newrow = pd.Series(newrow, index = list(vocab))\n",
        "    new = new.append(newrow, ignore_index = True)\n",
        "  \n",
        "  return new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LwCPr4O7Fc6y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Input: The Bag of Words Model\n",
        "#Output: Inverse Document Frequencies\n",
        "#Takes the log of each row if they are >0\n",
        "def IDF(DataFrame):\n",
        "  df = DataFrame.astype(bool).sum(axis=0) #document frequency\n",
        "  numDocs = int(DataFrame.shape[1])\n",
        "  idf = []\n",
        "  for elt in df:\n",
        "    if(elt != 0):\n",
        "      idf.append(math.log10(numDocs/elt))\n",
        "    else:\n",
        "      idf.append(0)\n",
        "  return idf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wSVizq_zF8AX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as py\n",
        "\n",
        "#Input: Outputs of LogFrequencies and IDF functions\n",
        "#Output: TFIDF feature\n",
        "#Multiplies every row of LogFrequencies with IDF\n",
        "def TFIDF(logF,idf):\n",
        "  tfidf = pd.DataFrame(data=None, index=None, columns=vocab, dtype=None, copy=False)\n",
        "  \n",
        "  for index, row in logF.iterrows():\n",
        "    newrow = py.multiply(row,idf)\n",
        "    \n",
        "    newrow = pd.Series(newrow, index = list(vocab))\n",
        "    tfidf = tfidf.append(newrow, ignore_index = True)\n",
        "  return tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XiKZhCO8PVRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compute the TFI-DF Features\n",
        "tfidf = TFIDF(LogFrequencies(bow),IDF(bow))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LqKxJC_ZTCQV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfidf.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4e2UyiYSvrs3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **3. Word2Vec**"
      ]
    },
    {
      "metadata": {
        "id": "0KhBsjhg8ivz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word2vec is a two-layer neural network that processes text. It's input is the sentences we have, and it outputs a feature vector for each word in that sentence. These vectors are called neural word embeddings. The key characteristic of this approach is that similar words have similar feature vectors. The size of the feature vectors can be specified, and we use the default size, which is 100. The process of extracting the feature vectors can be parallelized by increasing the *workers* parameter. The similarity of the words are calculated using cosine similarity."
      ]
    },
    {
      "metadata": {
        "id": "dI1ZNQ6tzGBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "55zlIeB7F22g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yelpProcessed = yelp\n",
        "myDataPreprocessing(yelpProcessed)\n",
        "yelpProcessed.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4WGk2lZG5TS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#build the model\n",
        "#word_vectors store the neural word embedding for each word\n",
        "model_w2v = gensim.models.Word2Vec(yelpProcessed['Sentence'], size=100, window=5, min_count=1, workers=4)\n",
        "word_vectors = model_w2v.wv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCwoqF3BHnBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#As an example, this is the neural word embedding of the word \"good\"\n",
        "model_w2v.wv['good']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2X5U3CGNr-xZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Using this function, it finds similar words with input, here \"good\" is tried again.\n",
        "model_w2v.wv.most_similar('good')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vGa9d7JiD8P3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_w2v.wv.most_similar('food')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Vy2OzMqOL0c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_w2v.wv.most_similar('bad')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u0yE7K-6RsN5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_w2v.wv.most_similar('nice')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgYWePVpQKCr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_w2v.wv.similarity('bad','not')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wfe7-U4PRclU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_w2v.wv.similarity('not','good')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIQmRff4tVWr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#With this function, it finds the most similar words for the positive set, and most dissimilar words for the negative set.\n",
        "model_w2v.wv.most_similar_cosmul(positive= ['good', 'best'],negative=['bad'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HQZSdOFOts3e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function finds the word which is the outlier in the word string.\n",
        "model_w2v.wv.doesnt_match(\"good best better bad\".split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9yvFBuu5OPUT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We can also save the model\n",
        "filename = \"w2v.txt\"\n",
        "model_w2v.wv.save_word2vec_format(filename, binary = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z61CHxAH57eb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Deep Learning**"
      ]
    },
    {
      "metadata": {
        "id": "uypU6mPI_KIP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can make predictions using the features that we have extracted. The first one is deep learning. In this part, we will form 3 networks using the Sequential model of Keras. The embedding layer will change in each case."
      ]
    },
    {
      "metadata": {
        "id": "j6BDCvbc6AxJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Using **Keras**"
      ]
    },
    {
      "metadata": {
        "id": "Z_c-04yt_im9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we will utilize the Embedding layer of Keras"
      ]
    },
    {
      "metadata": {
        "id": "Zn86WAKdUSjT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67s5sdl5_2Sh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the model to work, we need to tokenize every sentence. This means, each word will have a unique number assigned to it, and the sentence will be represented using those numbers instead of words. Moreover, the input has to be same length. This means, we would like to represent each sentence as if they had same number of words. To achieve this, we find the sentence having maximum length and pad the other sentences until they have this length."
      ]
    },
    {
      "metadata": {
        "id": "_r4mcXqMJ_8Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yelpProcessed.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Npb77VE4KQGs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yelp_processed_sentences = []\n",
        "for row in yelpProcessed['Sentence']:\n",
        "  str = \"\"\n",
        "  for item in row:\n",
        "    str += item + \" \"\n",
        "  yelp_processed_sentences.append(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6e4mYzaUlId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Tokenize every sentence\n",
        "tokenizer_obj = Tokenizer()\n",
        "total_reviews = yelp['Sentence']\n",
        "tokenizer_obj.fit_on_texts(total_reviews)\n",
        "\n",
        "#myList = []\n",
        "#for row in total_reviews:\n",
        "#  str = \"\"\n",
        "#  for item in row:\n",
        "#    str += item + \" \"\n",
        "#  myList.append(str)\n",
        "\n",
        "#Find the maximum length of a sentence = max_length\n",
        "#Vocab size = number of different words contained in the data\n",
        "max_length = max([len(s.split()) for s in yelp_processed_sentences])\n",
        "vocab_size = len(tokenizer_obj.word_index) +1\n",
        "\n",
        "#Tokenize and pad train and test data seperately.\n",
        "x_train_t = tokenizer_obj.texts_to_sequences(x_train['Sentence'])\n",
        "x_valid_t = tokenizer_obj.texts_to_sequences(x_valid['Sentence'])\n",
        "\n",
        "x_train_p = pad_sequences(x_train_t, maxlen = max_length, padding='post')\n",
        "x_valid_p = pad_sequences(x_valid_t, maxlen = max_length, padding='post')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2JezCAIMfHF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Max length of input sentence: \", max_length)\n",
        "print(\"Number of words in vocabulary: \",vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tggbIqQPA9J9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we will form the Sequential model. First layer will be the word embeddings. As an input, we put size of the vocabulary, size of feature vectors (100), and maximum input length (max_length). After this, we add a GRU layer, Gated Recurrent Unit, an improved version of standard recurrent neural network. Units, is the dimensionality of output space. And, we have a dense layer at the end. The actiavion function used is sigmiod as we are doing a binary classification."
      ]
    },
    {
      "metadata": {
        "id": "LA2Ce--AR0rA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "model_keras = Sequential()\n",
        "model_keras.add(Embedding(vocab_size, 100, input_length = max_length))\n",
        "model_keras.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "model_keras.add(Dense(1,activation= 'sigmoid'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9DTvDc2vCcUO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We configure the learning process with the compile method. The model will try to minimize the loss function. Binary Cross-Entropy is chosen as this is a binary classification. Cosine proximity is also tried."
      ]
    },
    {
      "metadata": {
        "id": "Gfil4XahTGyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_keras.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HyZV3MFjDDRV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The number of epochs, the number of times the network will update the weights, is set to 25 using empirical analysis. Batch is the number of samples in each gradient update and its default value is 25."
      ]
    },
    {
      "metadata": {
        "id": "-KhW3BigTW-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_keras.fit(x_train_p,y_train,epochs=25, batch_size=32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IAThJat7Da0m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The predictions are in a float format, te closer it gets to 1 means there is a high probability that this is positive, and vice versa. They are converted to 0 and 1 based on their probability"
      ]
    },
    {
      "metadata": {
        "id": "1LK6wLZfYD-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_keras_1 = model_keras.predict(x_valid_p)\n",
        "\n",
        "for i in range(len(pred_keras_1)):\n",
        "  if pred_keras_1[i] >= 0.5:\n",
        "    pred_keras_1[i] = 1\n",
        "  else:\n",
        "    pred_keras_1[i] = 0\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iefXhqWdDwoR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The labels are also converted to an appropriate format."
      ]
    },
    {
      "metadata": {
        "id": "W1JmNVxCY8S2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "labels = np.zeros(len(y_valid))\n",
        "for i in range(len(y_valid)):\n",
        "  labels[i] = int(y_valid.values[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tDEQkUM8D0aJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These are the results. The loss function did not make any impact on the result. "
      ]
    },
    {
      "metadata": {
        "id": "qrXWsMLmYeZe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 score for the negative class  \", f1_score(labels, pred_keras_1,average=None)[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-10QKeBZ6HXg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **2. Using Word2Vec**"
      ]
    },
    {
      "metadata": {
        "id": "dZSUN0ksXYR_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this part, instead of using Keras's Embedding layer, we will create our embedding layer using our word2vec model. We obtain it from the saved file, and rest is the same."
      ]
    },
    {
      "metadata": {
        "id": "sl3UdyZeOlHk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get feature vectors\n",
        "f2 = open(\"w2v.txt\",encoding=\"utf-8\")\n",
        "embeddings_index_2 = {}\n",
        "for line in f2:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:])\n",
        "  embeddings_index_2[word] = coefs\n",
        "f2.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JdkT6KgzQCNJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Construct the embedding matrix\n",
        "embedding_matrix_2 = py.zeros((vocab_size,100))\n",
        "\n",
        "for word, i in tokenizer_obj.word_index.items():\n",
        "  row = embeddings_index_2.get(word)\n",
        "  if row is not None:\n",
        "    embedding_matrix_2[i] = row\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RyyQKgMcQn00",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Construct the model\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "embeddingLayer = Embedding(vocab_size,100,embeddings_initializer=keras.initializers.Constant(embedding_matrix_2), input_length = max_length, trainable = False)\n",
        "\n",
        "model_nn_w2v = Sequential()\n",
        "model_nn_w2v.add(embeddingLayer)\n",
        "model_nn_w2v.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "model_nn_w2v.add(Dense(1,activation= 'sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5CpVZCR34azv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_nn_w2v.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "model_nn_w2v.fit(x_train_p,y_train,epochs=50, batch_size=50)\n",
        "preds_nn_w2v = model_nn_w2v.predict(x_valid_p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "smRine25H2O3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Form the labels from y_valid\n",
        "labels = np.zeros(len(y_valid))\n",
        "for i in range(len(y_valid)):\n",
        "  labels[i] = int(y_valid.values[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4F8JV2Fu4xwh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(preds_nn_w2v)):\n",
        "  if preds_nn_w2v[i] >= 0.5:\n",
        "    preds_nn_w2v[i] = 1\n",
        "  else:\n",
        "    preds_nn_w2v[i] = 0\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print(\" F1 score for the negative class: \",f1_score(labels, preds_nn_w2v, average=None)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vywE97mo6KR9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **3. Using GloVe**"
      ]
    },
    {
      "metadata": {
        "id": "jwrh7K0wXnyc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GloVe is also an algorithm to extract neural word embeddings. But it is pretrained. That is why we downloaded its pretrained 100 dimensional feature vectors and used it as the embedding layer. This might have better results than Word2Vec as it probably is trained with more data. As this is the best net, grid search was used to optimize the batch size and number of epochs."
      ]
    },
    {
      "metadata": {
        "id": "Ic3OJE6MZxkx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#If this does not work, please download glove.6B.zip and add the 100d file https://nlp.stanford.edu/projects/glove/\n",
        "#Get the neural word embeddings.\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = py.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uD22n7HCYwMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of words in GloVe vocabulary: \", len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0-GD8_R3aHJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Form the embedding matrix\n",
        "embedding_matrix = py.zeros((107331, 100))\n",
        "for word, i in tokenizer_obj.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SUeHUsj6qm0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  model_glove2 = Sequential()\n",
        "  e = Embedding(107331, 100, weights=[embedding_matrix], input_length=19, trainable=False)\n",
        "  model_glove2.add(e)\n",
        "  model_glove2.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "  model_glove2.add(Dense(1, activation='sigmoid'))\n",
        "  model_glove2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "  return model_glove2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yF8TOii-iNw9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                  results['mean_test_score'][candidate],\n",
        "                  results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KmdplwdVrGZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This takes very long due to Grid Search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "model_glove2 = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [10, 50, 100]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator=model_glove2, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(x_train_p, y_train)\n",
        "report(grid_result.cv_results_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PCXE0DkfaQiZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "\n",
        "#form the model\n",
        "model_glove = Sequential()\n",
        "e = Embedding(107331, 100, weights=[embedding_matrix], input_length=19, trainable=False)\n",
        "model_glove.add(e)\n",
        "model_glove.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "model_glove.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model_glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# summarize the model\n",
        "print(model_glove.summary())\n",
        "# fit the model\n",
        "model_glove.fit(x_train_p,y_train,epochs=50, batch_size=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TONHO-_xbOEO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Predict\n",
        "preds = model_glove.predict(x_valid_p)\n",
        "\n",
        "for i in range(len(preds)):\n",
        "  if preds[i] >= 0.5:\n",
        "    preds[i] = 1\n",
        "  else:\n",
        "    preds[i] = 0\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sinviFTqbOEY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Form the labels from y_valid\n",
        "labels = np.zeros(len(y_valid))\n",
        "for i in range(len(y_valid)):\n",
        "  labels[i] = int(y_valid.values[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "O-vvALPJbOEd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Analyse\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 score for the negative class \", f1_score(labels, preds,average=None)[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i-CtAlnr0Wc1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Multinomial Naive Bayes**"
      ]
    },
    {
      "metadata": {
        "id": "aF8ofOKkatPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will not use neural word embeddings with Naive Bayes as it does not allow negative values as an input. "
      ]
    },
    {
      "metadata": {
        "id": "xJdckGgKTWgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **1. Naive Bayes using Bag of Words**"
      ]
    },
    {
      "metadata": {
        "id": "hPd890bvQ4zp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train the Multinomial Naive Bayes Classifier with Bag of Words Features\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clfnb = MultinomialNB().fit(bow, y_train.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vWSvng5k3wc6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Prepare the test data for prediction by putting it into Bag of Words Format\n",
        "myDataPreprocessing(x_valid)\n",
        "bow_test = ComputeBOW(x_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0CbgDfX0Ru8I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Predict\n",
        "predicted = clfnb.predict(bow_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EIC_WDbNyzxm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 score for the negative class \", f1_score(labels, preds,average=None)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pWGXmmjVTq5s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **2. Naive Bayes using TFIDF**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6aKGtykLTwqo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train the Multinomial Naive Bayes Classifier with TFIDF features\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf2 = MultinomialNB().fit(tfidf, y_train.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j13j2jurT-I3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  #Prepare the test data for prediction by extracting its TFIDF features\n",
        "tfidf_test = TFIDF(LogFrequencies(bow_test),IDF(bow_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PKwAeS_wULjH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Predict\n",
        "predicted2 = clf2.predict(tfidf_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCo0ASRaUQ9a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Analyse\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print( \"F1 score for the negative class: \",f1_score(y_valid, predicted2, average=None)[0])\n",
        "print(\"Confusion Matrix: \",confusion_matrix(y_valid, predicted2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RUGcYeW5VzLK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Support Vector Machine**"
      ]
    },
    {
      "metadata": {
        "id": "85zMF4jM8_mD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The goal of SVM is to maximize the margin between classes. The main idea behind this is to map the data into a higher dimension. Thanks to the kernels it use, the data does not need to be direcly mapped to a higher dimension but the same outcome can be obtained without mapping. Word2Vec and GloVe also maps the words into high dimensions. That is why TFIDF is used as a feature rather than neural word embeddings. Also, Randomized Grid Search is used to optimize the hyperparameters."
      ]
    },
    {
      "metadata": {
        "id": "251ug9Afglaz",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#The execution takes long due to randomized search\n",
        "import scipy\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import sklearn\n",
        "clf3 = svm.SVC()\n",
        "param_dist = {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n",
        "  'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
        "rand_search = RandomizedSearchCV(clf3, param_distributions=param_dist,n_iter=20, cv=5)\n",
        "rand_search.fit(tfidf, y_train.values)\n",
        "report(rand_search.cv_results_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "teA8OnRWV3k5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "clf3 = svm.SVC(C=13.88933423371426, class_weight=None, gamma=0.015967055411381006).fit(tfidf, y_train.values)\n",
        "predicted3 = clf3.predict(tfidf_test)\n",
        "print(\"F1 score for the negative class: \",f1_score(y_valid, predicted3,average=None)[0])\n",
        "print(\"Confusion Matrix: \",confusion_matrix(y_valid, predicted3))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tbhuCsQ24KFw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# N-Gram"
      ]
    },
    {
      "metadata": {
        "id": "rFzO6WmDnRro",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "N-grams are simply all combinations of adjacent words or letters of length n that can be found in source text. Here, aim is to group the review sentences and see if there are any related words that may affect polarity of the review."
      ]
    },
    {
      "metadata": {
        "id": "hirAqQ_UNxnz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk, re, string, collections\n",
        "from nltk.util import ngrams # function for making ngrams\n",
        "from nltk.classify import NaiveBayesClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bDbYgS6SOZyR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train data is split into sentences and their corresponding labels\n",
        "\n",
        "x = yelp['Sentence']\n",
        "y = yelp['Value']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gP7wSkzP0OIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Function for creating n-grams. \n",
        "#This function is different from Natural Language Toolkit function ngrams() in terms of returning value.\n",
        "#It returns dictionary for n-grams to be used for Naive Bayes Classifier.\n",
        "\n",
        "def ngram(words, n):\n",
        "    ngram_vocab = ngrams(words, n)\n",
        "    my_dict = dict([(ng, True) for ng in ngram_vocab])\n",
        "    return my_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "86HYvaImkUQO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "------------------------------------------\n",
        "\n",
        "**Applying Naive Bayes**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "euXk4AfEycVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#1-gram with Naive Bayes Classifier\n",
        "data = []\n",
        "counter = 0\n",
        "for sentt in x:\n",
        "  words = sentt\n",
        "  if y.loc[counter] == '1':\n",
        "    data.append((ngram(words,1), \"positive\")) \n",
        "  if y.loc[counter] == '0':\n",
        "    data.append((ngram(words,1), \"negative\"))   \n",
        "  counter = counter +1\n",
        "    \n",
        "train_set = data[:800]\n",
        "test_set =  data[800:]\n",
        "classifier1 = nltk.NaiveBayesClassifier.train(train_set)\n",
        "accuracy = nltk.classify.util.accuracy(classifier1, test_set)\n",
        "print(\"Accuracy: \",accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "shjn3xDZs426",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier1.show_most_informative_features(32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FvwLAZwYFOrz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#2-gram with Naive Bayes Classifier\n",
        "data2 = []\n",
        "counter2 = 0\n",
        "for sentt2 in x:\n",
        "  words2 = sentt2\n",
        "  if y.loc[counter2] == '1':\n",
        "    data2.append((ngram(words2,2), 1)) \n",
        "  if y.loc[counter2] == '0':\n",
        "    data2.append((ngram(words2,2), 0))   \n",
        "  counter2 = counter2 +1\n",
        "    \n",
        "train_set2 = data2[:800]\n",
        "test_set2 =  data2[800:]\n",
        "classifier2 = NaiveBayesClassifier.train(train_set2)\n",
        "accuracy2 = nltk.classify.util.accuracy(classifier2, test_set2)\n",
        "print(\"Accuracy: \",accuracy2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aVeeFIyUFeOd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#3-gram with Naive Bayes Classifier\n",
        "data3 = []\n",
        "counter3 = 0\n",
        "for sentt3 in x:\n",
        "  words3 = sentt3\n",
        "  if y.loc[counter3] == '1':\n",
        "    data3.append((ngram(words3,3), \"positive\")) \n",
        "  if y.loc[counter3] == '0':\n",
        "    data3.append((ngram(words3,3), \"negative\"))   \n",
        "  counter3 = counter3 +1\n",
        "    \n",
        "train_set3 = data3[:800]\n",
        "test_set3 =  data3[800:]\n",
        "classifier3 = NaiveBayesClassifier.train(train_set3)\n",
        "accuracy3 = nltk.classify.util.accuracy(classifier3, test_set3)\n",
        "print(\"Accuracy: \",accuracy3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r8FUc-2rFhr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#4-gram with Naive Bayes Classifier\n",
        "data4 = []\n",
        "counter4 = 0\n",
        "for sentt4 in x:\n",
        "  words4 = sentt4\n",
        "  if y.loc[counter4] == '1':\n",
        "    data4.append((ngram(words4,4), \"positive\")) \n",
        "  if y.loc[counter4] == '0':\n",
        "    data4.append((ngram(words4,4), \"negative\"))   \n",
        "  counter4 = counter4 +1\n",
        "    \n",
        "train_set4 = data4[:800]\n",
        "test_set4 =  data4[800:]\n",
        "classifier4 = NaiveBayesClassifier.train(train_set4)\n",
        "accuracy4 = nltk.classify.util.accuracy(classifier4, test_set4)\n",
        "print(\"Accuracy: \",accuracy4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c2TuqOxwFpLe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#5-gram with Naive Bayes Classifier\n",
        "data5 = []\n",
        "counter5 = 0\n",
        "for sentt5 in x:\n",
        "  words5 = sentt5\n",
        "  if y.loc[counter5] == '1':\n",
        "    data5.append((ngram(words5,5), \"positive\")) \n",
        "  if y.loc[counter5] == '0':\n",
        "    data5.append((ngram(words5,5), \"negative\"))   \n",
        "  counter5 = counter5 +1\n",
        "    \n",
        "train_set5 = data5[:800]\n",
        "test_set5 =  data5[800:]\n",
        "classifier5 = NaiveBayesClassifier.train(train_set5)\n",
        "accuracy5 = nltk.classify.util.accuracy(classifier5, test_set5)\n",
        "print(\"Accuracy: \",accuracy5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1aB1yCTEldvh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since our dataset is very small, best result is obtained from 1-gram."
      ]
    },
    {
      "metadata": {
        "id": "pOl_gyEJ_7VC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Phrase Modelling "
      ]
    },
    {
      "metadata": {
        "id": "RnGhdb2RAAPU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another thing that can be implemented with Gensim library is phrase detection. It is similar to n-gram, but instead of getting all the n-gram by sliding the window, it detects frequently-used phrases and sticks them together. "
      ]
    },
    {
      "metadata": {
        "id": "MUa9-b054PO_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yelp.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cOAEfjengoNU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train data is split into sentences and their corresponding labels\n",
        "\n",
        "x = yelp['Sentence']\n",
        "y = yelp['Value']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QRA-UMBADHvN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Preprocessed sentences are converted to strings and inserted to a dataframe for tokenization.\n",
        "\n",
        "yelpDF = pd.DataFrame(data=None, index=None, columns=['Sentence']) \n",
        "\n",
        "for i in x:\n",
        "  new_string=\"\"\n",
        " \n",
        "  for word in i:\n",
        "    new_string+=(word+\" \")\n",
        "  row = pd.Series(new_string, index=['Sentence'])\n",
        "  yelpDF = yelpDF.append(row, ignore_index = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rcdXQ_0BKDB-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = yelpDF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vuZsR2F1JAW5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0shod1JY6Dh5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train1, x_validation1, y_train1, y_validation1 = train_test_split(X['Sentence'], y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "anE_sHccn5VU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating Bigram and Trigram"
      ]
    },
    {
      "metadata": {
        "id": "QQ97Iy3j6DKA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Importing Gensim Libraries\n",
        "import gensim.models.phrases\n",
        "from gensim.models.phrases import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "from gensim.models import  word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VWbwLpdAOpSb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Sentences are splitted for tokenization\n",
        "tokenized_train = [t.split() for t in x_train1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmI584IEQJ8N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_l = max([len(s.split()) for s in X['Sentence']])\n",
        "vocab_size1 = len(tokenizer_obj.word_index) +1\n",
        "\n",
        "#Tokenize and pad train and test data seperately.\n",
        "x_trainT = tokenizer_obj.texts_to_sequences(x_train1)\n",
        "x_validT = tokenizer_obj.texts_to_sequences(x_validation1)\n",
        "\n",
        "x_trainP = pad_sequences(x_trainT, maxlen = max_l, padding='post')\n",
        "x_validP = pad_sequences(x_validT, maxlen = max_l, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EfryN_zrAJnz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating bigrams and trigram using Phrases and Phraser from Gensim\n",
        "phrases = Phrases(tokenized_train) \n",
        "bigram = Phraser(phrases)\n",
        "trigram = Phrases(bigram[tokenized_train])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WmJw5FK5nWKW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Word2vec"
      ]
    },
    {
      "metadata": {
        "id": "Xnyo3YLiQYMh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Word2vec is trained with bigram\n",
        "bigram[tokenized_train]\n",
        "modelB = word2vec.Word2Vec(bigram[tokenized_train], size=100, min_count=10, iter=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0fUwEFCbRjIF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "modelB.wv.most_similar('good')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lvegxpDwSCEZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "modelB.wv.most_similar('not')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kn3T7Ry3SNmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "modelB.wv.most_similar('bad')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SICF3z-3MsS5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "filename = \"w2vBigram.txt\"\n",
        "modelB.wv.save_word2vec_format(filename, binary = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S4HHTVRXNZDV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get feature vectors\n",
        "fileB = open(\"w2vBigram.txt\",encoding=\"utf-8\")\n",
        "embeddings_index_B = {}\n",
        "for lineb in fileB:\n",
        "  valuesB = lineb.split()\n",
        "  wordB = valuesB[0]\n",
        "  coefsB = np.asarray(valuesB[1:])\n",
        "  embeddings_index_B[wordB] = coefsB\n",
        "fileB.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ozUFvducOKur",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Construct the embedding matrix\n",
        "embedding_matrix_B = py.zeros((vocab_size,100))\n",
        "\n",
        "for word, i in tokenizer_obj.word_index.items():\n",
        "  row = embeddings_index_B.get(word)\n",
        "  if row is not None:\n",
        "    embedding_matrix_B[i] = row\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cI0_RrJyOhmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Construct the model\n",
        "embeddingLayerB = Embedding(vocab_size,100,embeddings_initializer=keras.initializers.Constant(embedding_matrix_B), input_length = max_length, trainable = False)\n",
        "\n",
        "model_B_w2v = Sequential()\n",
        "model_B_w2v.add(embeddingLayerB)\n",
        "model_B_w2v.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "model_B_w2v.add(Dense(1,activation= 'sigmoid'))\n",
        "\n",
        "model_B_w2v.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "model_B_w2v.fit(x_trainP,y_train1,epochs=50, batch_size=50)\n",
        "preds_B_w2v = model_B_w2v.predict(x_validP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pD6-1YpdSUz4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Form the labels from y_valid\n",
        "labels1 = np.zeros(len(y_validation1))\n",
        "for i in range(len(y_validation1)):\n",
        "  labels1[i] = int(y_validation1.values[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_iQtkdeISkrY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(preds_B_w2v)):\n",
        "  if preds_B_w2v[i] >= 0.5:\n",
        "    preds_B_w2v[i] = 1\n",
        "  else:\n",
        "    preds_B_w2v[i] = 0\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 score for the negative class: \",f1_score(labels1, preds_B_w2v, average=None)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_h8LW3W63Oik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EGxpj5FToQOV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Word2vec is trained with trigram\n",
        "modelT = word2vec.Word2Vec(trigram[tokenized_train], size=100, min_count=10, iter=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pk29sq5oVC6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "modelT.wv.most_similar('good')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MDzTAMMwM2m_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "filename = \"w2vTrigram.txt\"\n",
        "modelT.wv.save_word2vec_format(filename, binary = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "95y6-H8JN3ox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get feature vectors\n",
        "fileT = open(\"w2vTrigram.txt\",encoding=\"utf-8\")\n",
        "embeddings_index_T = {}\n",
        "for linet in fileT:\n",
        "  valuesT = linet.split()\n",
        "  wordT = valuesT[0]\n",
        "  coefsT = np.asarray(valuesT[1:])\n",
        "  embeddings_index_T[wordT] = coefsT\n",
        "fileT.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rSIIRwALOUJ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Construct the embedding matrix\n",
        "embedding_matrix_T = py.zeros((vocab_size,100))\n",
        "\n",
        "for word, i in tokenizer_obj.word_index.items():\n",
        "  row = embeddings_index_T.get(word)\n",
        "  if row is not None:\n",
        "    embedding_matrix_T[i] = row\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_pkGyV4OtqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Construct the model\n",
        "embeddingLayerT = Embedding(vocab_size,100,embeddings_initializer=keras.initializers.Constant(embedding_matrix_T), input_length = max_length, trainable = False)\n",
        "\n",
        "model_T_w2v = Sequential()\n",
        "model_T_w2v.add(embeddingLayerT)\n",
        "model_T_w2v.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "model_T_w2v.add(Dense(1,activation= 'sigmoid'))\n",
        "\n",
        "model_T_w2v.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "model_T_w2v.fit(x_trainP,y_train1,epochs=50, batch_size=50)\n",
        "preds_T_w2v = model_T_w2v.predict(x_validP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BsHVAdzZTGQj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Form the labels from y_valid\n",
        "labels2 = np.zeros(len(y_validation1))\n",
        "for i in range(len(y_validation1)):\n",
        "  labels2[i] = int(y_validation1.values[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AOlUcns3TO0u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(preds_T_w2v)):\n",
        "  if preds_T_w2v[i] >= 0.5:\n",
        "    preds_T_w2v[i] = 1\n",
        "  else:\n",
        "    preds_T_w2v[i] = 0\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 score for the negative class: \",f1_score(labels2, preds_T_w2v, average=None)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQTr379Boyjp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Ensemble**"
      ]
    },
    {
      "metadata": {
        "id": "tyDAjpPn9TQq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We formed an ensemble using our best models to prevent overfitting and increase accuracy.\n",
        "\n",
        "\n",
        "\n",
        "1.   Bigram Word2Vec: *preds_B_w2v*\n",
        "2.   SVM Using DTFIF Features: *predicted3*\n",
        "3.   Trigam Word2Vec: *preds_T_w2v*\n",
        "4.   Naive Bayes with Bag Words Features: *predicted*\n",
        "5.   Lexicographic Approach: *preds_lex*\n",
        "6.   Using Glove Features with Neural Net:*preds*\n",
        "7.   Using Keras's Embedding Layer:*preds_keras_1*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jQue43Dto0JN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import statistics as st\n",
        "final_result = []\n",
        "\n",
        "#svm, nb bow, lex, glove, keras\n",
        "for i in range (0,200):\n",
        "  row = [int(preds_B_w2v[i]), int(predicted3[i]),int(preds_T_w2v[i]),int(predicted[i]), int(preds_lex[i]), int(preds[i,0]), int(pred_keras_1[i,0])];\n",
        "  final_result.append(st.mode(row))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "goGP0DhoCddu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_valid_2 = list(map(int, y_valid.values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uqt-bSVBCEJU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"F1 score for the negative class: \",f1_score(y_valid_2, final_result,average=None)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8PtaqnQ95evN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ** Final Test**"
      ]
    },
    {
      "metadata": {
        "id": "NeYxcYQD5vs8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will retrain the models used in our ensemble again including the validation data too, before the final test. The test_data will be read and prepared.."
      ]
    },
    {
      "metadata": {
        "id": "AAXnlHzq57el",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**1. Read and prepare the test data**"
      ]
    },
    {
      "metadata": {
        "id": "V-rEiae2g4jh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get the test file within Colab. Please upload the test file with the same name to the Files section on the left hand side.\n",
        "import pandas as pd\n",
        "TEST_DATA = pd.read_excel('yelp-test.xlsx',index_col=None, header=None,dtype={'0':str, '1':str})\n",
        "TEST_DATA = TEST_DATA.rename(columns={0: \"Sentence\", 1: \"Value\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "20Ni_r7IhkLT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Head of Test data\n",
        "TEST_DATA.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RE-L4amoADuU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Also divide the test data\n",
        "TEST_DATA_X = TEST_DATA.drop(['Value'],axis=1)\n",
        "TEST_DATA_Y = TEST_DATA['Value']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R3TAthtekGfF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Apply Preprocessing to Test Data\n",
        "myDataPreprocessing(TEST_DATA_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WRXazYW__LUY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2. Extract Bag of Words Features**"
      ]
    },
    {
      "metadata": {
        "id": "E7tXoRgvEctO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bowtest = ComputeBOW(TEST_DATA_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kt-NA_ym_OS9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3. Extract TFIDF Features**"
      ]
    },
    {
      "metadata": {
        "id": "D1jkUNl7E4ty",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfidftest = TFIDF(LogFrequencies(bowtest),IDF(bowtest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AE2qYU3r_URU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**4. Train the Moltinomial Naive Bayes with BOW**"
      ]
    },
    {
      "metadata": {
        "id": "ilgPBSeSEi-8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predicted_nb = clfnb.predict(bowtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I_eXjn0UJ4VW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predicted_nb_int = []\n",
        "for i in range(len(predicted_nb)):\n",
        "  predicted_nb_int.append(int(predicted_nb[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-NfDPFAjEvZ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 score for the negative class: \", f1_score(TEST_DATA_Y.values, predicted_nb_int,average=None)[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BQM-qxY5_YTD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**5. Train SVM with TFIDF**"
      ]
    },
    {
      "metadata": {
        "id": "WhQzq5VSEzis",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "predicted_svc = clf3.predict(tfidftest)\n",
        "\n",
        "predicted_svc_int = []\n",
        "for i in range(len(predicted_svc)):\n",
        "  predicted_svc_int.append(int(predicted_svc[i]))\n",
        "\n",
        "print(\"F1 score for the negative class: \",f1_score(TEST_DATA_Y.values, predicted_svc_int,average=None)[0])\n",
        "print(\"Confusion Matrix: \",confusion_matrix(TEST_DATA_Y.values, predicted_svc_int))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGd7slXT_goQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " **7. Train the Network using Keras's Embedding Layer**"
      ]
    },
    {
      "metadata": {
        "id": "mrOwTvdNNsAP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_sentences = pd.concat([TEST_DATA_X['Sentence'],yelpProcessed['Sentence']], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_DfyZJbpNcfn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_processed_sentences = []\n",
        "for row in all_sentences:\n",
        "  str = \"\"\n",
        "  for item in row:\n",
        "    str += item + \" \"\n",
        "  all_processed_sentences.append(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7zYB_EDwNcfr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Tokenize every sentence\n",
        "tokenizer_obj = Tokenizer()\n",
        "total_reviews = all_sentences\n",
        "tokenizer_obj.fit_on_texts(total_reviews)\n",
        "\n",
        "\n",
        "#Find the maximum length of a sentence = max_length\n",
        "#Vocab size = number of different words contained in the data\n",
        "max_length = max([len(s.split()) for s in all_processed_sentences])\n",
        "vocab_size = len(tokenizer_obj.word_index) +1\n",
        "\n",
        "#Tokenize and pad train and test data seperately.\n",
        "x_train_t = tokenizer_obj.texts_to_sequences(x_train['Sentence'])\n",
        "x_test_t = tokenizer_obj.texts_to_sequences(TEST_DATA_X['Sentence'])\n",
        "\n",
        "x_train_p = pad_sequences(x_train_t, maxlen = max_length, padding='post')\n",
        "x_test_p = pad_sequences(x_test_t, maxlen = max_length, padding='post')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WNq8rEJ3Ncft",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Max length of input sentence: \", max_length)\n",
        "print(\"Number of words in vocabulary: \",vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "84daT0vGNcfz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "model_keras_t = Sequential()\n",
        "model_keras_t.add(Embedding(vocab_size, 100, input_length = max_length))\n",
        "model_keras_t.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "model_keras_t.add(Dense(1,activation= 'sigmoid'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BfY8oODxNcf1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_keras_t.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hGRlC5SzNcf4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_keras_t.fit(x_train_p,y_train,epochs=25, batch_size=32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "j7GScWv-Ncf7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_keras_t1 = model_keras_t.predict(x_test_p)\n",
        "\n",
        "for i in range(len(pred_keras_t1)):\n",
        "  if pred_keras_t1[i] >= 0.5:\n",
        "    pred_keras_t1[i] = 1\n",
        "  else:\n",
        "    pred_keras_t1[i] = 0\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wY3MdWAKNcgC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 score for the negative class: \", f1_score(TEST_DATA_Y, pred_keras_t1,average=None)[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pMpGh7HkRMlP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**8.Glove**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-Qo2dqYIRFwZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#If this does not work, please download glove.6B.zip and add the 100d file https://nlp.stanford.edu/projects/glove/\n",
        "#Get the neural word embeddings.\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = py.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qSTJEM3zRFwp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Form the embedding matrix\n",
        "embedding_matrix = py.zeros((107331, 100))\n",
        "for word, i in tokenizer_obj.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rLAXumxfRFw7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  model_glove2 = Sequential()\n",
        "  e = Embedding(107331, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
        "  model_glove2.add(e)\n",
        "  model_glove2.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "  model_glove2.add(Dense(1, activation='sigmoid'))\n",
        "  model_glove2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "  return model_glove2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vBh4rxkQRFw_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "\n",
        "#form the model\n",
        "model_glove = Sequential()\n",
        "e = Embedding(107331, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
        "model_glove.add(e)\n",
        "model_glove.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "model_glove.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model_glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# summarize the model\n",
        "print(model_glove.summary())\n",
        "# fit the model\n",
        "model_glove.fit(x_train_p,y_train,epochs=50, batch_size=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "16Ccsx_NRFxG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Predict\n",
        "preds_glv = model_glove.predict(x_test_p)\n",
        "\n",
        "for i in range(len(preds_glv)):\n",
        "  if preds_glv[i] >= 0.5:\n",
        "    preds_glv[i] = 1\n",
        "  else:\n",
        "    preds_glv[i] = 0\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yfMX58W3RFxL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Analyse\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 score for the negative class: \", f1_score(TEST_DATA_Y, preds_glv,average=None)[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHiIVjaHDk2f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**9. Predict with Lexicon Based Approach**"
      ]
    },
    {
      "metadata": {
        "id": "CH3D5M1GDpOj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds_lex = []\n",
        "for row in TEST_DATA['Sentence']:\n",
        "    preds = swn_polarity(row)\n",
        "    preds_lex.append(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4XRbufFT4kze",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**10. N-Gram**"
      ]
    },
    {
      "metadata": {
        "id": "dBY_Y5qK4jaN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xlast = TEST_DATA_X['Sentence']  \n",
        "ylast = TEST_DATA_Y  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7hnXw1kY5ktv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#1-gram with Naive Bayes Classifier\n",
        "datatest = []\n",
        "countertest = 0\n",
        "for senttest in xlast:\n",
        "  wordstest = senttest\n",
        "  if ylast.loc[countertest] == '1':\n",
        "    datatest.append((ngram(wordstest,1), \"positive\")) \n",
        "  if ylast.loc[countertest] == '0':\n",
        "    datatest.append((ngram(wordstest,1), \"negative\"))   \n",
        "  countertest = countertest +1   \n",
        "\n",
        "#classifier1 is from train data (1-gram)\n",
        "accuracytest = nltk.classify.util.accuracy(classifier1, datatest)\n",
        "print('1-gram Naive Bayes Classifier accuracy: ',accuracytest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pmnd3MSaugP9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**11. Phrase Modelling**"
      ]
    },
    {
      "metadata": {
        "id": "-JSdaEFauhWh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#bigram with word2vec testing\n",
        "\n",
        "model_B_test = Sequential()\n",
        "model_B_test.add(Embedding(vocab_size, 100, input_length = max_length))\n",
        "model_B_test.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "model_B_test.add(Dense(1,activation= 'sigmoid'))\n",
        "model_B_test.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "model_B_test.fit(x_train_p,y_train,epochs=25, batch_size=32)\n",
        "preds_B_w2v_test = model_B_test.predict(x_test_p)\n",
        "\n",
        "for i in range(len(preds_B_w2v_test)):\n",
        "  if preds_B_w2v_test[i] >= 0.5:\n",
        "    preds_B_w2v_test[i] = 1\n",
        "  else:\n",
        "    preds_B_w2v_test[i] = 0\n",
        "\n",
        "scoreB = f1_score(TEST_DATA_Y, preds_B_w2v_test,average=None)[0]\n",
        "print(\"Word2vec model score trained with bigram: \", scoreB)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e9GbiCIEulPy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#trigram with word2vec testing\n",
        "\n",
        "model_T_test = Sequential()\n",
        "model_T_test.add(Embedding(vocab_size, 100, input_length = max_length))\n",
        "model_T_test.add(GRU(units=32, dropout= 0.2, recurrent_dropout = 0.2))\n",
        "model_T_test.add(Dense(1,activation= 'sigmoid'))\n",
        "model_T_test.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "model_T_test.fit(x_train_p,y_train,epochs=25, batch_size=32)\n",
        "preds_T_w2v_test = model_T_test.predict(x_test_P)\n",
        "\n",
        "for i in range(len(preds_T_w2v_test)):\n",
        "  if preds_T_w2v_test[i] >= 0.5:\n",
        "    preds_T_w2v_test[i] = 1\n",
        "  else:\n",
        "    preds_T_w2v_test[i] = 0\n",
        "\n",
        "scoreT = f1_score(TEST_DATA_Y, preds_T_w2v_test,average=None)[0]\n",
        "print(\"Word2vec model score trained with trigram: \", scoreT)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7pWly9N_sEM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**12. Form the Ensemle adding abovementioned models**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PFSagsaDSDrh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import statistics as st\n",
        "final_result = []\n",
        "\n",
        "#svm, nb bow, lex, glove, keras, bigram with word2vec, trigram with word2vec\n",
        "for i in range (0,len(TEST_DATA_Y)):\n",
        "  row = [int(preds_B_w2v_test[i]),int(preds_T_w2v_tes[i]),int(predicted_nb_int[i]), int(predicted_svc_int[i]), int(preds_lex[i]), int(preds_glv[i,0]), int(pred_keras_t1[i,0])];\n",
        "  final_result.append(st.mode(row))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p2W5IMsHSDro",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"F1 score for the negative class: \" ,f1_score(TEST_DATA_Y, final_result,average=None)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWQrIiN8o0cQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "metadata": {
        "id": "2G4LKUTto2N3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis is a process of automatically identifying whether a user-generated text expresses positive, negative or neutral opinion about an entity (i.e. product, people, topic, event etc).Sentiment classification can be done at Document level, Sentence level and Aspect or Feature level. In Document level the whole document is used as a basic information unit to classify it either into positive or negative class. Sentence level sentiment classification classifies each sentence first as subjective or objective and then classifies into positive, negative or neutral class. There is no much difference between the above two methods as sentence is just a short document. Aspect or Feature level sentiment classification deals with identifying and extracting product features from the source data.\n",
        "\n",
        "After doing a lexicon based approach, we have done feature extraction and used several ML algorithms. The maximum F1 score that we have got in our train data was 0.82 with Naive Bayes using Bag of words. Also our ensemble gave 0.82 score. The test score is significantly lower than train score, presumably.\n",
        "\n",
        "Since the dataset is very small, the other approaches did not significantly improve "
      ]
    }
  ]
}